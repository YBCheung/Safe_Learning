{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Lyapunov function for Inverted Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\anaconda3\\envs\\tf115\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# from dreal import *\n",
    "from Functions import *\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import timeit \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import gymnasium as gym\n",
    "from gym.envs.classic_control import PendulumEnv\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.env_util import DummyVecEnv\n",
    "# Check if a GPU is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network model\n",
    "Building NN with random parameters for Lyapunov function and initializing parameters of NN controller to LQR solution\n",
    "\n",
    "LQR solution is obtained by minimizing the cost function J = ∫(xᵀQx + uᵀRu)dt, where Q is 2×2 identity matrix and R is 1×1 identity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,n_input,n_hidden,n_output):\n",
    "        super(Net, self).__init__()\n",
    "        torch.manual_seed(2)\n",
    "        self.layer1 = nn.Linear(n_input, n_hidden)\n",
    "        self.layer2 = nn.Linear(n_hidden,n_output)\n",
    "        # self.control = torch.nn.Linear(n_input,1,bias=False)\n",
    "        # self.control.weight = torch.nn.Parameter(lqr)\n",
    "\n",
    "    def forward(self,x):\n",
    "        sigmoid = nn.Tanh()\n",
    "        h_1 = sigmoid(self.layer1(x))\n",
    "        out = sigmoid(self.layer2(h_1))\n",
    "        # u = self.control(x)\n",
    "        return out\n",
    "    \n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size=2, output_size=1):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)  # Input size: 3 (3D float input), Output size: 64\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc6 = nn.Linear(32, output_size)  # Output size: 1 (1D float output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc6(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Pendulum-v1 environment\n",
    "env = PendulumEnv()\n",
    "policy_model = DDPG.load(\"ddpg_pendulum_10k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamical system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_model = NeuralNetwork(3,2)\n",
    "dynamic_model.load_state_dict(torch.load('model_2_NN2.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For learning \n",
    "'''\n",
    "N = 500             # sample size\n",
    "N_test = 10\n",
    "D_in = 2            # input dimension\n",
    "H1 = 6              # hidden dimension\n",
    "D_out = 1           # output dimension\n",
    "torch.manual_seed(10)\n",
    "# x_ = torch.Tensor(N, D_in).uniform_(-6, 6)\n",
    "x1 = torch.Tensor(N, 1).uniform_(-np.pi, np.pi)\n",
    "x2 = torch.Tensor(N, 1).uniform_(-8,8)\n",
    "x = torch.cat((x1, x2), dim=1)\n",
    "x_0 = torch.zeros([1, 2])\n",
    "\n",
    "epsilon = 0\n",
    "# Checking candidate V within a ball around the origin (ball_lb ≤ sqrt(∑xᵢ²) ≤ ball_ub)\n",
    "ball_lb = 0.5\n",
    "ball_ub = 74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning and Falsification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Lyapunov Risk= 51.195308685302734\n",
      "V: [0.06327085 0.07257738 0.1285501  0.13085963 0.12798062 0.11326627\n",
      " 0.09432318 0.06615569 0.06330413 0.08255624] \n",
      "L_V: [ 0.0525274  -0.06110699 -0.01994774  0.00501661 -0.03717655 -0.11200157\n",
      " -0.16435948 -0.01050313  0.08809321  0.00071695]\n",
      "unsatisfied sample number: 4 / 10\n",
      "100 Lyapunov Risk= 6.215770244598389\n",
      "V: [0.41820142 0.5409999  0.79727644 0.3681025  0.8101883  0.7639698\n",
      " 0.7444578  0.5201716  0.8097011  0.8113744 ] \n",
      "L_V: [-0.03275818  0.11352111  0.15692435  0.02038437 -0.06771705  0.34876513\n",
      "  0.08955294  0.5028398  -0.07557252 -0.05658466]\n",
      "unsatisfied sample number: 6 / 10\n",
      "200 Lyapunov Risk= 3.6875855922698975\n",
      "V: [0.16560881 0.16265887 0.88526976 0.47945842 0.8272452  0.91507107\n",
      " 0.43549445 0.8926228  0.34511945 0.7897651 ] \n",
      "L_V: [0.24701162 0.12413705 0.1179286  0.3975738  0.22080636 0.05126626\n",
      " 0.26117796 0.09060867 0.7182742  0.61447626]\n",
      "unsatisfied sample number: 10 / 10\n",
      "300 Lyapunov Risk= 3.2875163555145264\n",
      "V: [0.86347586 0.89470553 0.8511608  0.8676815  0.5990942  0.6531313\n",
      " 0.40847042 0.9280272  0.74970776 0.8596132 ] \n",
      "L_V: [0.34747866 0.15840293 0.24553794 0.261258   0.52260494 0.3655598\n",
      " 0.3992635  0.08723839 0.76345164 0.33330032]\n",
      "unsatisfied sample number: 10 / 10\n",
      "400 Lyapunov Risk= 3.0805015563964844\n",
      "V: [0.38093618 0.37975803 0.9545444  0.462159   0.8803681  0.8967232\n",
      " 0.7473655  0.9310151  0.95773286 0.9106223 ] \n",
      "L_V: [0.3500897  0.27215013 0.08330802 0.5325882  0.39782682 0.18553388\n",
      " 0.39989862 0.12351795 0.08829573 0.20888422]\n",
      "unsatisfied sample number: 10 / 10\n",
      "500 Lyapunov Risk= 2.953411102294922\n",
      "V: [0.8017094  0.64372885 0.9453621  0.9609961  0.4108097  0.9535008\n",
      " 0.2942591  0.92338073 0.7529929  0.17836854] \n",
      "L_V: [0.37334406 0.67208165 0.12412004 0.07780197 0.48822358 0.09446205\n",
      " 0.2765965  0.20379815 0.32704103 0.08176564]\n",
      "unsatisfied sample number: 10 / 10\n",
      "600 Lyapunov Risk= 2.8744680881500244\n",
      "V: [0.9545009  0.9217381  0.30435947 0.9506665  0.6424899  0.90250874\n",
      " 0.9425586  0.43316036 0.9653611  0.1695742 ] \n",
      "L_V: [0.11796618 0.2463131  0.40039963 0.10334835 0.57187355 0.23958042\n",
      " 0.19233279 0.513871   0.07257573 0.11825341]\n",
      "unsatisfied sample number: 10 / 10\n",
      "700 Lyapunov Risk= 2.821711778640747\n",
      "V: [0.27432904 0.95820254 0.12303911 0.94629234 0.7339698  0.8744957\n",
      " 0.34876585 0.6969393  0.41249117 0.96634   ] \n",
      "L_V: [0.31096143 0.11767886 0.02536137 0.15727669 0.45099926 0.4370547\n",
      " 0.3139632  0.6700814  0.45021316 0.0988434 ]\n",
      "unsatisfied sample number: 9 / 10\n",
      "800 Lyapunov Risk= 2.782475709915161\n",
      "V: [0.7611481  0.4695282  0.8397249  0.74507415 0.9518806  0.9140495\n",
      " 0.8878107  0.72255135 0.9646011  0.21823372] \n",
      "L_V: [0.68171084 0.5537197  0.43216223 0.5341578  0.16074489 0.24516186\n",
      " 0.40218413 0.6240288  0.10184266 0.17785317]\n",
      "unsatisfied sample number: 10 / 10\n",
      "900 Lyapunov Risk= 2.7473201751708984\n",
      "V: [0.5377135  0.2516149  0.48792306 0.40246606 0.9820246  0.4075577\n",
      " 0.91832596 0.983329   0.9429662  0.3635629 ] \n",
      "L_V: [0.77777004 0.3036334  0.6753623  0.4480829  0.06334125 0.42852724\n",
      " 0.26353246 0.06264747 0.19073473 0.44586647]\n",
      "unsatisfied sample number: 10 / 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25552\\2000365993.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLyapunov_risk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mLyapunov_risk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dell\\anaconda3\\envs\\tf115\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[0;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         )\n\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dell\\anaconda3\\envs\\tf115\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m def grad(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "out_iters = 0\n",
    "valid = False\n",
    "while out_iters < 2 and not valid: \n",
    "    start = timeit.default_timer()\n",
    "    # lqr = torch.tensor([[-23.58639732,  -5.31421063]])    # lqr solution\n",
    "    model = Net(D_in,H1, D_out)\n",
    "    L = []\n",
    "    i = 0 \n",
    "    t = 0\n",
    "    max_iters = 2000\n",
    "    learning_rate = 0.01\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    while i < max_iters and not valid: \n",
    "        V_candidate = model(x)\n",
    "        X0 = model(x_0)\n",
    "        x_obs = torch.cat((np.cos(x[:,0]).reshape(-1,1), np.sin(x[:,0]).reshape(-1,1), x[:,1].reshape(-1,1)), dim=1)\n",
    "        u, _ = policy_model.predict(x_obs)\n",
    "        # print(type(x), type(u), type(torch.cat((x,torch.from_numpy(u)), dim=1)))\n",
    "        f = dynamic_model(torch.cat((x,torch.from_numpy(u)), dim=1))\n",
    "        # f = f_value(x,u)\n",
    "        Circle_Tuning = Tune(x)\n",
    "        # Compute lie derivative of V : L_V = ∑∂V/∂xᵢ*fᵢ\n",
    "        L_V = torch.diagonal(torch.mm(torch.mm(torch.mm(dtanh(V_candidate),model.layer2.weight)\\\n",
    "                            *dtanh(torch.tanh(torch.mm(x,model.layer1.weight.t())+model.layer1.bias)),model.layer1.weight),f.t()),0)\n",
    "\n",
    "        # With tuning term \n",
    "        Lyapunov_risk = (F.relu(-V_candidate)+ 1.5*F.relu(L_V+0.5)).mean()\\\n",
    "                    +2.2*((Circle_Tuning-6*V_candidate).pow(2)).mean()+(X0).pow(2)\n",
    "        # Without tuning term\n",
    "#         Lyapunov_risk = (F.relu(-V_candidate)+ 1.5*F.relu(L_V+0.5)).mean()+ 1.2*(X0).pow(2)\n",
    "        \n",
    "        \n",
    "        # print(i, \"Lyapunov Risk=\",Lyapunov_risk.item())\n",
    "        L.append(Lyapunov_risk.item())\n",
    "        optimizer.zero_grad()\n",
    "        Lyapunov_risk.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Falsification\n",
    "        if i % 100 == 0:\n",
    "\n",
    "            print(i, \"Lyapunov Risk=\",Lyapunov_risk.item())\n",
    "            x1_test = torch.Tensor(N_test, 1).uniform_(-np.pi, np.pi)\n",
    "            x2_test = torch.Tensor(N_test, 1).uniform_(-8,8)\n",
    "            x_test = torch.cat((x1_test, x2_test), dim=1)\n",
    "            x_obs_test = torch.cat((np.cos(x1_test), np.sin(x1_test), x2_test), dim=1)\n",
    "\n",
    "            V_candidate = model(x_test)\n",
    "            u, _ = policy_model.predict(x_obs_test)\n",
    "            f = dynamic_model(torch.cat((x_test,torch.from_numpy(u)), dim=1))\n",
    "            # Compute lie derivative of V : L_V = ∑∂V/∂xᵢ*fᵢ\n",
    "            L_V = torch.diagonal(torch.mm(torch.mm(torch.mm(dtanh(V_candidate),model.layer2.weight)\\\n",
    "                            *dtanh(torch.tanh(torch.mm(x_test,model.layer1.weight.t())+model.layer1.bias)),model.layer1.weight),f.t()),0)\n",
    "            index = np.logical_and(V_candidate.reshape(-1)>0, L_V<epsilon)\n",
    "            print(\"V:\",V_candidate.detach().numpy().reshape(-1),'\\nL_V:' , L_V.detach().numpy())          \n",
    "\n",
    "            # print('===========Verifying==========')\n",
    "            start_ = timeit.default_timer()\n",
    "            result= CheckLyapunov(x_test, f, V_candidate.reshape(-1), L_V, ball_lb, ball_ub, epsilon)\n",
    "            stop_ = timeit.default_timer()\n",
    "\n",
    "            print(\"unsatisfied sample number:\",result.size()[0],\"/\",N_test)\n",
    "            if (result.size()[0]!=0): \n",
    "                # print(\"Not a Lyapunov function. Found counterexample: \")\n",
    "                # print(result)\n",
    "                # x = AddCounterexamples(x,result,10)\n",
    "                pass\n",
    "            else:  \n",
    "                valid = True\n",
    "                print(\"Satisfy conditions!!\")\n",
    "                print(V_candidate, \" is a Lyapunov function.\")\n",
    "            t += (stop_ - start_)\n",
    "            # print('==============================')\n",
    "        i += 1\n",
    "\n",
    "    stop = timeit.default_timer()\n",
    "\n",
    "\n",
    "    torch.save(model.state_dict(), 'Lyapunov_NN_gym.pth')\n",
    "    print('\\n')\n",
    "    print(\"Total time: \", stop - start)\n",
    "    print(\"Verified time: \", t)\n",
    "    \n",
    "    out_iters+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5759,  2.6159],\n",
      "        [-2.3344, -1.7129],\n",
      "        [ 2.6748, -5.9888],\n",
      "        [ 0.6697, -0.2034],\n",
      "        [ 2.8404,  3.5864]]) [[-2.       ]\n",
      " [-2.       ]\n",
      " [-1.988219 ]\n",
      " [-1.9999838]\n",
      " [ 2.       ]] tensor([[ 0.7629,  2.6442],\n",
      "        [-2.3715, -2.3223],\n",
      "        [ 2.4222, -6.0445],\n",
      "        [ 0.6482, -0.2884],\n",
      "        [ 3.0235,  3.9638]], grad_fn=<SliceBackward0>) tensor([0.4476, 0.3885, 0.1300, 0.0636, 0.7706], grad_fn=<SliceBackward0>)\n",
      "0 Lyapunov Risk= 2.502589225769043\n",
      "tensor([[ 0.5759,  2.6159],\n",
      "        [-2.3344, -1.7129],\n",
      "        [ 2.6748, -5.9888],\n",
      "        [ 0.6697, -0.2034],\n",
      "        [ 2.8404,  3.5864]]) [[-2.       ]\n",
      " [-2.       ]\n",
      " [-1.988219 ]\n",
      " [-1.9999838]\n",
      " [ 2.       ]] tensor([[ 0.7629,  2.6442],\n",
      "        [-2.3715, -2.3223],\n",
      "        [ 2.4222, -6.0445],\n",
      "        [ 0.6482, -0.2884],\n",
      "        [ 3.0235,  3.9638]], grad_fn=<SliceBackward0>) tensor([0.4647, 0.4049, 0.1168, 0.0623, 0.7748], grad_fn=<SliceBackward0>)\n",
      "1 Lyapunov Risk= 2.507005214691162\n"
     ]
    }
   ],
   "source": [
    "model = Net(D_in,H1, D_out)\n",
    "model.load_state_dict(torch.load('Lyapunov_NN_gym.pth'))\n",
    "L = []\n",
    "i = 0 \n",
    "t = 0\n",
    "max_iters = 2000\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "while i < 2 and not valid: \n",
    "    V_candidate = model(x)\n",
    "    X0 = model(x_0)\n",
    "    u, _ = policy_model.predict(x_obs)\n",
    "    # print(type(x), type(u), type(torch.cat((x,torch.from_numpy(u)), dim=1)))\n",
    "    f = dynamic_model(torch.cat((x,torch.from_numpy(u)), dim=1))\n",
    "    # f = f_value(x,u)\n",
    "    Circle_Tuning = Tune(x)\n",
    "    # Compute lie derivative of V : L_V = ∑∂V/∂xᵢ*fᵢ\n",
    "    L_V = torch.diagonal(torch.mm(torch.mm(torch.mm(dtanh(V_candidate),model.layer2.weight)\\\n",
    "                        *dtanh(torch.tanh(torch.mm(x,model.layer1.weight.t())+model.layer1.bias)),model.layer1.weight),f.t()),0)\n",
    "\n",
    "    # With tuning term \n",
    "    Lyapunov_risk = (F.relu(-V_candidate)+ 1.5*F.relu(L_V+0.5)).mean()\\\n",
    "                +2.2*((Circle_Tuning-6*V_candidate).pow(2)).mean()+(X0).pow(2)\n",
    "    # Without tuning term\n",
    "#         Lyapunov_risk = (F.relu(-V_candidate)+ 1.5*F.relu(L_V+0.5)).mean()+ 1.2*(X0).pow(2)\n",
    "    print(x[:5], u[:5], f[:5], L_V[:5])\n",
    "    \n",
    "    print(i, \"Lyapunov Risk=\",Lyapunov_risk.item())\n",
    "    L.append(Lyapunov_risk.item())\n",
    "    optimizer.zero_grad()\n",
    "    Lyapunov_risk.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    w1 = model.layer1.weight.data.numpy()\n",
    "    w2 = model.layer2.weight.data.numpy()\n",
    "    b1 = model.layer1.bias.data.numpy()\n",
    "    b2 = model.layer2.bias.data.numpy()\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking result with smaller epsilon ( Lie derivative of V <= epsilon )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Satisfy conditions with epsilon= ', -1e-05)\n",
      "(<Expression \"tanh((0.49927884340286255 - 0.41210386157035828 * tanh((-0.79895162582397461 - 0.053098957985639572 * x1 + 0.046276744455099106 * x2)) + 0.82691246271133423 * tanh((-0.6677706241607666 + 0.69698750972747803 * x1 - 0.0021623193752020597 * x2)) - 0.9570583701133728 * tanh((0.83614975214004517 + 0.9072798490524292 * x1 + 0.013366221450269222 * x2)) - 0.059784829616546631 * tanh((0.90165179967880249 + 0.16413372755050659 * x1 - 0.36351147294044495 * x2)) + 0.97691076993942261 * tanh((1.1444443464279175 + 0.028182908892631531 * x1 - 0.026499949395656586 * x2)) - 0.27872595191001892 * tanh((1.3038069009780884 - 0.43813130259513855 * x1 - 0.24401682615280151 * x2))))\">, ' is a Lyapunov function.')\n"
     ]
    }
   ],
   "source": [
    "epsilon = -0.00001\n",
    "start_ = timeit.default_timer() \n",
    "result = CheckLyapunov(vars_, f, V_learn, ball_lb, ball_ub, config, epsilon)\n",
    "stop_ = timeit.default_timer() \n",
    "\n",
    "if (result): \n",
    "    print(\"Not a Lyapunov function. Found counterexample: \")\n",
    "else:  \n",
    "    print(\"Satisfy conditions with epsilon= \",epsilon)\n",
    "    print(V_learn, \" is a Lyapunov function.\")\n",
    "t += (stop_ - start_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More details on Lyapunov risk\n",
    "Generally, we start training with Lyapunov risk without the tuning term.      \n",
    "For example, (1* F.relu(-V_candidate)+ 1.5* F.relu(L_V+0.5)).mean()+ 1.2*(X0).pow(2)    \n",
    "The weight of each term (1, 1.5, 1.2) can be tuned for balancing each Lyapunov condition.     \n",
    "Furthermore, using F.relu(L_V+0.5) allows the learning procedure to seek a candidate Lyapunov function with more negative Lie derivative.   \n",
    "Here 0.5 is also a tunable parameter based on your goal.    \n",
    "In this example, we use Lyapunov risk with tuning term for achieving large ROA     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
