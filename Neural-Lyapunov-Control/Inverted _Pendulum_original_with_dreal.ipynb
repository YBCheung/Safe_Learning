{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Lyapunov function for Inverted Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\anaconda3\\envs\\tf115\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# from dreal import *\n",
    "from Functions import *\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import timeit \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network model\n",
    "Building NN with random parameters for Lyapunov function and initializing parameters of NN controller to LQR solution\n",
    "\n",
    "LQR solution is obtained by minimizing the cost function J = ∫(xᵀQx + uᵀRu)dt, where Q is 2×2 identity matrix and R is 1×1 identity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,n_input,n_hidden,n_output,lqr):\n",
    "        super(Net, self).__init__()\n",
    "        torch.manual_seed(2)\n",
    "        self.layer1 = torch.nn.Linear(n_input, n_hidden)\n",
    "        self.layer2 = torch.nn.Linear(n_hidden,n_output)\n",
    "        self.control = torch.nn.Linear(n_input,1,bias=False)\n",
    "        self.control.weight = torch.nn.Parameter(lqr)\n",
    "\n",
    "    def forward(self,x):\n",
    "        sigmoid = torch.nn.Tanh()\n",
    "        h_1 = sigmoid(self.layer1(x))\n",
    "        out = sigmoid(self.layer2(h_1))\n",
    "        u = self.control(x)\n",
    "        return out,u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamical system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_value(x,u):\n",
    "    #Dynamics\n",
    "    y = []\n",
    "    G = 9.81  # gravity\n",
    "    L = 0.5   # length of the pole \n",
    "    m = 0.15  # ball mass\n",
    "    b = 0.1   # friction\n",
    "    \n",
    "    for r in range(0,len(x)): \n",
    "        f = [ x[r][1], \n",
    "              (m*G*L*np.sin(x[r][0])- b*x[r][1]) / (m*L**2)]\n",
    "        y.append(f) \n",
    "    y = torch.tensor(y)\n",
    "    y[:,1] = y[:,1] + (u[:,0]/(m*L**2))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For learning \n",
    "'''\n",
    "N = 500             # sample size\n",
    "D_in = 2            # input dimension\n",
    "H1 = 6              # hidden dimension\n",
    "D_out = 1           # output dimension\n",
    "torch.manual_seed(10)  \n",
    "x = torch.Tensor(N, D_in).uniform_(-6, 6)           \n",
    "x_0 = torch.zeros([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "For verifying \n",
    "'''\n",
    "# x1 = Variable(\"x1\")\n",
    "# x2 = Variable(\"x2\")\n",
    "# vars_ = [x1,x2]\n",
    "G = 9.81 \n",
    "l = 0.5  \n",
    "m = 0.15\n",
    "b = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = Config()\n",
    "# config.use_polytope_in_forall = True\n",
    "# config.use_local_optimization = True\n",
    "# config.precision = 1e-2\n",
    "# epsilon = 0\n",
    "# Checking candidate V within a ball around the origin (ball_lb ≤ sqrt(∑xᵢ²) ≤ ball_ub)\n",
    "ball_lb = 0.5\n",
    "ball_ub = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning and Falsification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True False] [ True  True  True] [ True  True  True] 50006\n"
     ]
    }
   ],
   "source": [
    "a = 10\n",
    "b = 12\n",
    "i = a<b\n",
    "j = a+3<b\n",
    "k = i and j\n",
    "# print(i,j, k)\n",
    "\n",
    "x = np.array(([1,2],[0,1], [100,200]))\n",
    "ball = x[:,0] * x[:,0] + x[:,1] * x[:,1]\n",
    "ball = np.square(x[:,0]) + np.square(x[:,1])\n",
    "i =  ball < 10\n",
    "j = ball > 0\n",
    "print(i, j, np.logical_or(i, j), ball.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'D_in' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20500\\723767203.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mlqr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m23.58639732\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;33m-\u001b[0m\u001b[1;36m5.31421063\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m# lqr solution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD_in\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mH1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD_out\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlqr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'D_in' is not defined"
     ]
    }
   ],
   "source": [
    "out_iters = 0\n",
    "valid = False\n",
    "while out_iters < 2 and not valid: \n",
    "    start = timeit.default_timer()\n",
    "    lqr = torch.tensor([[-23.58639732,  -5.31421063]])    # lqr solution\n",
    "    model = Net(D_in,H1, D_out,lqr)\n",
    "    L = []\n",
    "    i = 0 \n",
    "    t = 0\n",
    "    max_iters = 2000\n",
    "    learning_rate = 0.01\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    while i < max_iters and not valid: \n",
    "        V_candidate, u = model(x)\n",
    "        X0,u0 = model(x_0)\n",
    "        f = f_value(x,u)\n",
    "        # print(1, x[:3], 2, u[:3],3, f[:3],4, V_candidate[:3])\n",
    "        Circle_Tuning = Tune(x)\n",
    "        # Compute lie derivative of V : L_V = ∑∂V/∂xᵢ*fᵢ\n",
    "        L_V = torch.diagonal(torch.mm(torch.mm(torch.mm(dtanh(V_candidate),model.layer2.weight)\\\n",
    "                            *dtanh(torch.tanh(torch.mm(x,model.layer1.weight.t())+model.layer1.bias)),model.layer1.weight),f.t()),0)\n",
    "        \n",
    "        print(torch.mm(torch.mm(torch.mm(dtanh(V_candidate),model.layer2.weight)\\\n",
    "                            *dtanh(torch.tanh(torch.mm(x,model.layer1.weight.t())+model.layer1.bias)),model.layer1.weight),f.t()).size())\n",
    "        print(L_V.size())\n",
    "        # With tuning term \n",
    "        Lyapunov_risk = (F.relu(-V_candidate)+ 1.5*F.relu(L_V+0.5)).mean()\\\n",
    "                    +2.2*((Circle_Tuning-6*V_candidate).pow(2)).mean()+(X0).pow(2) \n",
    "        # Without tuning term\n",
    "#         Lyapunov_risk = (F.relu(-V_candidate)+ 1.5*F.relu(L_V+0.5)).mean()+ 1.2*(X0).pow(2)\n",
    "        \n",
    "        \n",
    "        print(i, \"Lyapunov Risk=\",Lyapunov_risk.item()) \n",
    "        L.append(Lyapunov_risk.item())\n",
    "        optimizer.zero_grad()\n",
    "        Lyapunov_risk.backward()\n",
    "        optimizer.step() \n",
    "\n",
    "        w1 = model.layer1.weight.data.numpy()\n",
    "        w2 = model.layer2.weight.data.numpy()\n",
    "        b1 = model.layer1.bias.data.numpy()\n",
    "        b2 = model.layer2.bias.data.numpy()\n",
    "        q = model.control.weight.data.numpy()\n",
    "\n",
    "        # Falsification\n",
    "        # if i % 10 == 0:\n",
    "        #     u_NN = (q.item(0)*x1 + q.item(1)*x2) \n",
    "        #     f = [ x2,\n",
    "        #          (m*G*l*sin(x1) + u_NN - b*x2) /(m*l**2)]\n",
    "\n",
    "        #     # Candidate V\n",
    "        #     z1 = np.dot(vars_,w1.T)+b1\n",
    "\n",
    "        #     a1 = []\n",
    "        #     for j in range(0,len(z1)):\n",
    "        #         a1.append(tanh(z1[j]))\n",
    "        #     z2 = np.dot(a1,w2.T)+b2\n",
    "        #     V_learn = tanh(z2.item(0))\n",
    "\n",
    "        #     print('===========Verifying==========')        \n",
    "        #     # start_ = timeit.default_timer() \n",
    "        #     # result= CheckLyapunov(vars_, f, V_learn, ball_lb, ball_ub, config,epsilon)\n",
    "        #     # stop_ = timeit.default_timer() \n",
    "\n",
    "        #     if (result): \n",
    "        #         print(\"Not a Lyapunov function. Found counterexample: \")\n",
    "        #         print(result)\n",
    "        #         x = AddCounterexamples(x,result,10)\n",
    "        #     else:  \n",
    "        #         valid = True\n",
    "        #         print(\"Satisfy conditions!!\")\n",
    "        #         print(V_learn, \" is a Lyapunov function.\")\n",
    "        #     t += (stop_ - start_)\n",
    "        #     print('==============================') \n",
    "        i += 1\n",
    "\n",
    "    stop = timeit.default_timer()\n",
    "    \n",
    "    # torch.save(model.state_dict(), 'Lyapunov_NN.pth')\n",
    "\n",
    "    print('\\n')\n",
    "    print(\"Total time: \", stop - start)\n",
    "    print(\"Verified time: \", t)\n",
    "    \n",
    "    out_iters+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor([[-0.5030, -0.2057],\n",
      "        [-2.2500,  1.3803],\n",
      "        [-3.4326, -1.0581]]) \n",
      " 2 tensor([[  8.5099],\n",
      "        [-11.6274],\n",
      "        [ 50.6047]], grad_fn=<SliceBackward0>) \n",
      " 3 tensor([[-2.0572e-01,  2.1802e+02],\n",
      "        [ 1.3803e+00, -3.2901e+02],\n",
      "        [-1.0581e+00,  1.3579e+03]], grad_fn=<SliceBackward0>) \n",
      " 4 tensor([[0.1276],\n",
      "        [0.3477],\n",
      "        [0.6908]], grad_fn=<SliceBackward0>)\n",
      "0 Lyapunov Risk= 0.9358062744140625\n",
      "1 tensor([[-0.5030, -0.2057],\n",
      "        [-2.2500,  1.3803],\n",
      "        [-3.4326, -1.0581]]) \n",
      " 2 tensor([[  8.5069],\n",
      "        [-11.6637],\n",
      "        [ 50.5809]], grad_fn=<SliceBackward0>) \n",
      " 3 tensor([[-2.0572e-01,  2.1794e+02],\n",
      "        [ 1.3803e+00, -3.2998e+02],\n",
      "        [-1.0581e+00,  1.3573e+03]], grad_fn=<SliceBackward0>) \n",
      " 4 tensor([[0.1365],\n",
      "        [0.3730],\n",
      "        [0.6610]], grad_fn=<SliceBackward0>)\n",
      "1 Lyapunov Risk= 0.9809844493865967\n"
     ]
    }
   ],
   "source": [
    "model = Net(D_in,H1, D_out,lqr)\n",
    "model.load_state_dict(torch.load('Lyapunov_NN.pth'))\n",
    "L = []\n",
    "i = 0\n",
    "t = 0\n",
    "max_iters = 2000\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "while i < 2 and not valid: \n",
    "    V_candidate, u = model(x)\n",
    "    X0,u0 = model(x_0)\n",
    "    f = f_value(x,u)\n",
    "    print(1, x[:3],'\\n', 2, u[:3],'\\n',3, f[:3],'\\n',4, V_candidate[:3])\n",
    "    Circle_Tuning = Tune(x)\n",
    "    # Compute lie derivative of V : L_V = ∑∂V/∂xᵢ*fᵢ\n",
    "    L_V = torch.diagonal(torch.mm(torch.mm(torch.mm(dtanh(V_candidate),model.layer2.weight)\\\n",
    "                        *dtanh(torch.tanh(torch.mm(x,model.layer1.weight.t())+model.layer1.bias)),model.layer1.weight),f.t()),0)\n",
    "\n",
    "    # With tuning term \n",
    "    Lyapunov_risk = (F.relu(-V_candidate)+ 1.5*F.relu(L_V+0.5)).mean()\\\n",
    "                +2.2*((Circle_Tuning-6*V_candidate).pow(2)).mean()+(X0).pow(2) \n",
    "    # Without tuning term\n",
    "#         Lyapunov_risk = (F.relu(-V_candidate)+ 1.5*F.relu(L_V+0.5)).mean()+ 1.2*(X0).pow(2)\n",
    "    \n",
    "    \n",
    "    print(i, \"Lyapunov Risk=\",Lyapunov_risk.item()) \n",
    "    L.append(Lyapunov_risk.item())\n",
    "    optimizer.zero_grad()\n",
    "    Lyapunov_risk.backward()\n",
    "    optimizer.step() \n",
    "\n",
    "    w1 = model.layer1.weight.data.numpy()\n",
    "    w2 = model.layer2.weight.data.numpy()\n",
    "    b1 = model.layer1.bias.data.numpy()\n",
    "    b2 = model.layer2.bias.data.numpy()\n",
    "    q = model.control.weight.data.numpy()\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking result with smaller epsilon ( Lie derivative of V <= epsilon )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vars_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6244\\1057850548.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m0.00001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mstart_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCheckLyapunov\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvars_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV_learn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mball_lb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mball_ub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mstop_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vars_' is not defined"
     ]
    }
   ],
   "source": [
    "epsilon = -0.00001\n",
    "start_ = timeit.default_timer() \n",
    "result = CheckLyapunov(vars_, f, V_learn, ball_lb, ball_ub, config, epsilon)\n",
    "stop_ = timeit.default_timer() \n",
    "\n",
    "if (result): \n",
    "    print(\"Not a Lyapunov function. Found counterexample: \")\n",
    "else:  \n",
    "    print(\"Satisfy conditions with epsilon= \",epsilon)\n",
    "    print(V_learn, \" is a Lyapunov function.\")\n",
    "t += (stop_ - start_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More details on Lyapunov risk\n",
    "Generally, we start training with Lyapunov risk without the tuning term.      \n",
    "For example, (1* F.relu(-V_candidate)+ 1.5* F.relu(L_V+0.5)).mean()+ 1.2*(X0).pow(2)    \n",
    "The weight of each term (1, 1.5, 1.2) can be tuned for balancing each Lyapunov condition.     \n",
    "Furthermore, using F.relu(L_V+0.5) allows the learning procedure to seek a candidate Lyapunov function with more negative Lie derivative.   \n",
    "Here 0.5 is also a tunable parameter based on your goal.    \n",
    "In this example, we use Lyapunov risk with tuning term for achieving large ROA     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf115",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
